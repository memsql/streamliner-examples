// This file contains transformers for processing log files generated by S3
package com.memsql.streamliner.examples

import com.memsql.spark.etl.api._
import com.memsql.spark.etl.utils.PhaseLogger
import org.apache.spark.rdd._
import org.apache.spark.sql._
import org.apache.spark.sql.types._

class S3AccessLogsTransformer extends SimpleByteArrayTransformer {
  val S3_LINE_LOGPATS = """(\S+) (\S+) \[(.*?)\] (\S+) (\S+) (\S+) (\S+) (\S+) (?:"([^"]+)"|-) (\S+) (\S+) (\S+) (\S+) (\S+) (\S+) (?:"([^"]+)"|-) (?:"([^"]+)"|-) (\S+)\s*""".r

  val SCHEMA = StructType(Array(
    StructField("bucket_owner", StringType, true),
    StructField("bucket", StringType, true),
    StructField("datetime", TimestampType, true),
    StructField("ip", StringType, true),
    StructField("requestor_id", StringType, true),
    StructField("request_id", StringType, true),
    StructField("operation", StringType, true),
    StructField("path", StringType, true),
    StructField("http_method_uri_proto", StringType, true),
    StructField("http_status", IntegerType, true),
    StructField("s3_error", StringType, true),
    StructField("bytes_sent", IntegerType, true),
    StructField("object_size", IntegerType, true),
    StructField("total_time", IntegerType, true),
    StructField("turn_around_time", IntegerType, true),
    StructField("referer", StringType, true),
    StructField("user_agent", StringType, true),
    StructField("version_id", StringType, true)
  ))

  val PARSER = new java.text.SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss +SSSS")

  def parseDateTime(s: String) = {
    new java.sql.Timestamp(PARSER.parse(s).getTime())
  }

  override def transform(sqlContext: SQLContext, rdd: RDD[Array[Byte]], config: UserTransformConfig, logger: PhaseLogger): DataFrame = {
    val stringRDD = rdd.map(byteUtils.bytesToUTF8String)
    val parsedRDD = stringRDD.flatMap(x => S3_LINE_LOGPATS.findAllMatchIn(x).map(y => y.subgroups))
    val timestampedRDD = parsedRDD.map(r => r.zipWithIndex.map({case (x, i) => if (i == 2) parseDateTime(x) else x}))
    val rowRDD = timestampedRDD.map(x => Row.fromSeq(x))
    sqlContext.createDataFrame(rowRDD, SCHEMA)
  }
}
